{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# Artificial Neural Networks with Keras\n",
    "\n",
    "FINALLY - lets goooo - Neural Networks, I've come to bargain!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/dj/whhvx2nj1jx6fqgjkcwy_1fc0000gn/T/ipykernel_73023/3321905234.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.rc('axes', labelsize=14)\n",
    "mpl.rc('xtick', labelsize=12)\n",
    "mpl.rc('ytick', labelsize=12)\n",
    "\n",
    "np.random.seed(24)\n",
    "\n",
    "# Ignore useless warnings (see SciPy issue #5998)\n",
    "import warnings\n",
    "warnings.filterwarnings(action=\"ignore\", message=\"^internal gelsd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Things to look into:\n",
    "\n",
    "Callbacks\n",
    "Early Stopping\n",
    "TensorBoard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From biological to artificial neurons\n",
    "\n",
    "Why this time ANNs might be good:\n",
    "- huge quantity of data available, and ANNs outperform other ML algorithms on complex problems\n",
    "- huge increase in computing powers since the 90s - Moore's law + gaming industry for the GPU cards\n",
    "- better training algorithms\n",
    "- limitations of ANN are benigh in practice\n",
    "- virtuous circle of funding and progress\n",
    "\n",
    "\n",
    "#### The perceptron\n",
    "\n",
    "Based on a threshold logic unit (TLU) - inputs/outputs are not numbers and inputs have weight\n",
    "\n",
    "Computing a weighted sum of its inputs, and then applying a step function to it.\n",
    "h(x) = step(z), where z = w1x1 + w2x2... = x^Tw\n",
    "\n",
    "TLU similar to logistic regression or linear SVM. Computes a linear combination of inputs, and if exceeding a threshold outputing the positive class, else negative. Can be used for simple binary classification.\n",
    "\n",
    "Perceptron has a single layer of TLUs, and all are connected to all the inputs. When all neurons in a layer are connected to all neurons of previous layer, it's called a fully connected layer or dense layer. \n",
    "\n",
    "#### How to train perceptron:\n",
    "\n",
    "Inspired by Hebb's rule - connection weights between 2 neurons increases when they have the same output. The training uses this rule and takes into account the network error and reduces it. The perceptron fed 1 training instance at a time, and for each instance makes predictions. For output neurons making wrong predictions, it reinforces the eonnction weights from the iputs that would have helped to make it correct.  \n",
    "\n",
    "Output neuron decision boundaries are linear, so they can't learn complex pattersn (like logistic regression classifiers). \n",
    "\n",
    "\n",
    "Many expected highly from perceptrons, but it's similar to stochastic gradient descent - incapable of solving some trivial problems like simple linear classification models. Many, disappointed, dropped neural networks altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data[:, (2, 3)]\n",
    "y = (iris.target == 0).astype(np.int)\n",
    "\n",
    "per_clf = Perceptron()\n",
    "per_clf.fit(X, y)\n",
    "\n",
    "per_clf.predict([[2, 0.5]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-layer Perceptron and Backpropagation\n",
    "\n",
    "MLP contains one input layer, one or more layers of TLUs (hidden layers) and one final layer of TLUs called the output layer. Every layer, (except output) contains a bias neuron and is fully connected to the next layer. \n",
    "\n",
    "Signal flows in one direction - example of feedforward neural network (FNN)\n",
    "\n",
    "Backpropagation - 1986, gradient descent using an effecient technique computing the gradients automatically. It can find out how each connection weight and bias term should be tweaked in order to reduce the error. Once gradients are found, it just performs a regular Gradient Descent step, and the whole process is repeated until the network converges to the solution. \n",
    "\n",
    "Auto gradient computation is called automatic differentiation, or autodiff. Backpropagation uses reverse-mode autodiff - fas/precise and suited when function differentiates many variables with few outputs.\n",
    "\n",
    "\n",
    "<br>\n",
    "<font color='green'><b>BACKPROPAGATION:</b></font>\n",
    "\n",
    "<font color='blue'> \n",
    "<br>\n",
    "    \n",
    "- 1 mini-batch at a time. Goes through the training set multiple times - each pass = epoch\n",
    "- Each mini-batch passed from one layer to the next - forward pass - same as making predictions\n",
    "- Algorithm measures output error - uses loss function comparing desired output with actual output\n",
    "- Computes how much each output connection contributed to that error - applying the chain rule\n",
    "- Measures how error contributions came from each connection in the layer below - using the chain rule again, until it reaches the input layer. It measures the error gradient across all connection weights by propagating the error gradient backward through the network.\n",
    "- Algorithm performs Gradient Descent, tweaking all connection weights in tehe network, using the error gradients just computed.\n",
    "</font>\n",
    "\n",
    "<br>\n",
    "Summary:\n",
    "\n",
    "- Prediction for each training instance\n",
    "- Measures error\n",
    "- Goes back to layers in reverse measuring the error contribution from each connection\n",
    "- Tweaks the connection weights to reduce error\n",
    "\n",
    "\n",
    "<b>Must randomly initilaize the weights.</b>\n",
    "\n",
    "For backpropagation to work, the step function of the MLP was replaced with a logistic function. (sigmoid=a type of logistic function)\n",
    "\n",
    "Other activation functions:\n",
    "- Hyperbolictangent function - S-shaped, continuous and differntiable - output ranges from -1 to 1. Speeds up convergence.\n",
    "- Rectified Linear Unit function - continuous but not differentiable at 0 - fast to compute - no maximum output and reduces Gradient Descent issues.\n",
    "\n",
    "\n",
    "Why we have activation functions:\n",
    "\n",
    "If chain contains only linear transformations, like f(x)=3x+2, g(x)=10x, then the output is still linear. Without non-linearity, the stack of layers is still a single layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression MLPs\n",
    "\n",
    "A single prediction only needs a single output neuron. For multivariate regressions (multiple values), one output neuron is required for every output dimension. E.g centre of image requires 2 output neurons (x,y coordinates).\n",
    "\n",
    "For regression, activation functions are not needed, so they can output any range of values. The softplus activation function makes sure otuput are always positive, and the logistic or hyperbolic tangent function guarantees the outputs to always fall within a given range of values, by scaling the labels to the appropirate range. \n",
    "\n",
    "The loss function is typically the mean squared error. The mean absolute error used when lots of outliers. Huber Loss is the combination of both. \n",
    "\n",
    "Hashtag means number of.\n",
    "\n",
    "| Hyperparameter | Typical Value |\n",
    "| :-- | :-- |\n",
    "| # input neurons | One per input feature (e.g., 28 x 28 = 784 for MNIST) |\n",
    "| #Â hidden layers | Depends on the problem. Typically 1 to 5. |\n",
    "| # neurons per hidden layer | Depends on the problem. Typically 10 to 100. |\n",
    "| output neurons | 1 per prediction dimension |\n",
    "| Hidden activation | ReLU (or SELU, see Chapter 11) |\n",
    "| Output activation | None or ReLU/Softplus (if positive outputs) or Logistic/Tanh (if bounded outputs) |\n",
    "| Loss function | MSE or MAE/Huber (if outliers) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification MLPs\n",
    "\n",
    "For binary classification, just a single output neuron using the logistic activation function is good, estimating a probability. MLPs also great with multilabel binary classification tasks, just needing more output neurons. \n",
    "\n",
    "Multiclass classification, is when each instance belongs only to a single class. One output neuron per class and using the softmax activation function. \n",
    "\n",
    "Since predicting probability distributions, cross-entropy is usually good for the loss function. <br>\n",
    "\n",
    "\n",
    "| Hyperparameter | Binary classification | Multilabel binary classification | Multiclass classifiation |\n",
    "| :-- | :-- | :-- | :-- |\n",
    "| Input and hidden layers | Same as regression | Same as regression | Same as regression |\n",
    "| # output neurons | 1 |  1 per label | 1 per class |\n",
    "| Output layer activation | Logistic | Logistic | Softmax |\n",
    "| Loss function | Cross-Entropy | Cross-Entropy | Cross-Entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing MLPs with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(24)\n",
    "tf.random.set_seed(24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Classifier - Sequential API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist = keras.datasets.fashion_mnist\n",
    "(X_train_full, y_train_full), (X_test, y_test) = fashion_mnist.load_data()\n",
    "\n",
    "# Image represented as 28*28 array instead of 1D array of 784\n",
    "X_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are using Gradient Descent, so must scale input features. For simplicity scaling the pixel intensities down\n",
    "# to 0~1 range by division by 255.0.\n",
    "\n",
    "# Validation Sets\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', 'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle Boot']\n",
    "\n",
    "class_names[y_train[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_rows = 4\n",
    "n_cols = 10\n",
    "plt.figure(figsize=(n_cols * 1.2, n_rows * 1.2))\n",
    "for row in range(n_rows):\n",
    "    for col in range(n_cols):\n",
    "        index = n_cols * row + col\n",
    "        plt.subplot(n_rows, n_cols, index + 1)\n",
    "        plt.imshow(X_train[index], cmap=\"binary\", interpolation=\"nearest\")\n",
    "        plt.axis('off')\n",
    "        plt.title(class_names[y_train[index]], fontsize=12)\n",
    "plt.subplots_adjust(wspace=0.2, hspace=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating model using the Sequential API\n",
    "\n",
    "model = keras.models.Sequential() # A sequential model - simpliest - networks with single stack of layers\n",
    "model.add(keras.layers.Flatten(input_shape=[28, 28])) # Input - convert each image into 1D array - simple preprocessing - could also add keras.layers.InputLayer as first layer\n",
    "model.add(keras.layers.Dense(300, activation='relu')) # 2 Hidden\n",
    "model.add(keras.layers.Dense(100, activation='relu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax')) # Output - 10 neurons since one per class - using softmax because classes are exclusive\n",
    "\n",
    "# Alternative method\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=[28, 28]),\n",
    "    keras.layers.Dense(300, activation=\"relu\"),\n",
    "    keras.layers.Dense(100, activation=\"relu\"),\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# If using keras.io will need to change imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first hidden layer has 784 * 300 connection weights, with 300 bias tersm, adding up to 235500 parameters. A lot of flexibility to train the data, but could also overfit when there's not much training data.\n",
    "\n",
    "Quite important to specify the input shape. Otherwise will need to wait - over-complication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[1].name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.get_layer('dense_3').name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights, biases = model.layers[1].get_weights()\n",
    "\n",
    "print(weights.shape)\n",
    "weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(biases.shape)\n",
    "biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dense layer initialized the connection weights randomly (as required), and biases initialized to zeros. Use kernel_initilaizer for a different initialization method, or bias_initializer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Compiling the Model\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy']) # Can also specify extra metrics to compute during training/evaluation\n",
    "\n",
    "# Full list of losses, optimizers and metrics\n",
    "# https://keras.io/api/losses/\n",
    "# https://keras.io/api/optimizers/\n",
    "# https://keras.io/api/metrics/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the sparse categorical crossentrophy oss since we have sparse labels (for each instance there is target classindex), and classes are exclusive. We can use keras.utils.to_categorical to convert sparse to one-hot vector labels. Vise versa use ng.argmax() with axis=1.\n",
    "\n",
    "For optimizer, sgd means we train the model using simple Stochastic Gradient Descent - Keras will do backpropagation (reverse-mode autodiff + Gradient Descent). There are more effecient optimizers.\n",
    "\n",
    "Since it's classifier, useful to specify accuracy in training/evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data = (X_valid, y_valid))\n",
    "\n",
    "# Here 1719 is not the num of training samples but num of batches. It is default to 32 - 55000/32=1718.75."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NEURAL NETWORK TRAINED!\n",
    "\n",
    "Each epoch, number of instances processed so far, mean training time, the loss and accuracy. With more epochs training loss decreases, and validation accuracy reaches 95%! (8% increase than book). \n",
    "\n",
    "Can also use validation_split instead of passing the entire validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(8, 5))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]\n",
    "y_proba = model.predict(X_new)\n",
    "y_proba.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict_classes(X_new)\n",
    "np.array(class_names)[y_pred]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression MLP - Sequential API\n",
    "\n",
    "All of the writing is lost due to Jupyter Notebook not autosaving. See actual Book for the details again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(housing.data, housing.target, random_state=42)\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train_full, y_train_full, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.transform(X_valid)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "X_new = X_test[:3]\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pd.DataFrame(history.history))\n",
    "plt.grid(True)\n",
    "plt.gca().set_ylim(0, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functinoal API\n",
    "\n",
    "More complex toplogies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(24)\n",
    "tf.random.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mean_squared_error\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "history = model.fit(X_train, y_train, epochs=20,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if you want to send different subsets of input features through the wide or deep paths? We will send 5 features (features 0 to 4), and 6 through the deep path (features 2 to 7). Note that 3 features will go through both (features 2, 3 and 4)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"output\")(concat)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "X_train_A, X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]\n",
    "X_new_A, X_new_B = X_test_A[:3], X_test_B[:3]\n",
    "\n",
    "history = model.fit((X_train_A, X_train_B), y_train, epochs=20,\n",
    "                    validation_data=((X_valid_A, X_valid_B), y_valid))\n",
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)\n",
    "y_pred = model.predict((X_new_A, X_new_B))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auxiliary output for regularization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name=\"wide_input\")\n",
    "input_B = keras.layers.Input(shape=[6], name=\"deep_input\")\n",
    "hidden1 = keras.layers.Dense(30, activation=\"relu\")(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation=\"relu\")(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name=\"main_output\")(concat)\n",
    "aux_output = keras.layers.Dense(1, name=\"aux_output\")(hidden2)\n",
    "\n",
    "model = keras.models.Model(inputs=[input_A, input_B],\n",
    "                           outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=[\"mse\", \"mse\"], loss_weights=[0.9, 0.1], optimizer=keras.optimizers.SGD(lr=1e-3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit([X_train_A, X_train_B], [y_train, y_train], epochs=20,\n",
    "                    validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test])\n",
    "\n",
    "y_pred_main, y_pred_aux = model.predict([X_new_A, X_new_B])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dynamic Models - Subclassing API\n",
    "\n",
    "Back on track again.\n",
    "\n",
    "Both Sequential/Functional are declarative - # of layers.\n",
    "Advantages: easily saved/shared, structure easily displayed, errors can be caught early, easy to debug, etc.\n",
    "But it's static. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WideAndDeepModel(keras.models.Model):\n",
    "    \n",
    "    def __init__(self, units=30, activation='relu', **kwargs):\n",
    "        super().__init__(**kwargs) #handles standard arguments\n",
    "        self.hidden1 = keras.layers.Dense(units, activation=activation)\n",
    "        self.hidden2 = keras.layers.Dense(units, activation=activation)\n",
    "        self.main_output = keras.layers.Dense(1)\n",
    "        self.aux_output = keras.layers.Dense(1)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        input_A, input_B = inputs\n",
    "        hidden1 = self.hidden1(input_B)\n",
    "        hidden2 = self.hidden2(hidden1)\n",
    "        concat = keras.layers.concatenate([input_A, hidden2])\n",
    "        main_output = self.main_output(concat)\n",
    "        aux_output = self.aux_output(hidden2)\n",
    "        \n",
    "        return main_output, aux_output\n",
    "    \n",
    "model = WideAndDeepModel()\n",
    "\n",
    "\n",
    "# Unlimited possibilites within the call function. Great with experimentations.\n",
    "# However, more difficult to inspect, save or clone. Cannot check and easiler to make mistakes. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving/Restoring model\n",
    "\n",
    "model.save(\"model_name\")\n",
    "\n",
    "model = keras.models.load_model(\"model_name\")\n",
    "\n",
    "If model takes long time to train, don't just save model at end but save checkpoints. Use Callbacks:\n",
    "\n",
    "\n",
    "### Using Callbacks\n",
    "\n",
    "Fit accepts callbacks argument letting Keras call during training, at start/end, start/end of each epoch and start/end of processing each batch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "np.random.seed(24)\n",
    "tf.random.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomly training\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation=\"relu\", input_shape=[8]),\n",
    "    keras.layers.Dense(30, activation=\"relu\"),\n",
    "    keras.layers.Dense(1)\n",
    "])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"my_keras_model.h5\")\n",
    "model = keras.models.load_model(\"my_keras_model.h5\")\n",
    "# Saving and reloading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doing the same thing, but with callbacks:\n",
    "\n",
    "keras.backend.clear_session()\n",
    "np.random.seed(24)\n",
    "tf.random.set_seed(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"mse\", optimizer=keras.optimizers.SGD(lr=1e-3))\n",
    "\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"my_keras_model.h5\", save_best_only=True)\n",
    "\n",
    "history = model.fit(X_train, y_train, epochs=10,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[checkpoint_cb])\n",
    "\n",
    "model = keras.models.load_model(\"my_keras_model.h5\") # rollback to best model\n",
    "mse_test = model.evaluate(X_test, y_test)\n",
    "\n",
    "# Another way of doing it is with early stopping. Can also write custom callbacks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization Using TensorBoard\n",
    "\n",
    "View learning curve during training, compare learning curves, visualize computation graph, etcetc.\n",
    "\n",
    "Modify program - output data wanting to visualize to special binary log file. Point TensorBoard server to root log directory, and configure program for different subdirectory saving. This way you can visualize/compare data from multiple runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining root log directory - current time so always different\n",
    "\n",
    "root_logdir = os.path.join(os.curdir, \"my_logs\")\n",
    "\n",
    "def get_run_logdir():\n",
    "    import time\n",
    "    run_id = time.strftime('run_%Y_%m_%d-%H_%M_%S')\n",
    "    return os.path.join(root_logdir, run_id)\n",
    "\n",
    "run_logdir = get_run_logdir()\n",
    "\n",
    "# TensorBoard callback\n",
    "\n",
    "# After building/compiling model\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "history = model.fit(X_train, y_train, epochs=30,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    callbacks=[tensorboard_cb])\n",
    "\n",
    "\n",
    "# USEFUL THING TO DO - look into this later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fine-Tuning Hyperparameters\n",
    "\n",
    "# Approach 1 - which works best on validation set - GridSearch\n",
    "\n",
    "def build_model(n_hidden=1, n_neurons=30, learning_rate=3e-3, input_shape=[8]):\n",
    "    model = keras.models.Sequential()\n",
    "    options = {\"input_shape\": input_shape}\n",
    "    for layer in range(n_hidden):\n",
    "        model.add(keras.layers.Dense(n_neurons, activation=\"relu\", **options))\n",
    "        options = {}\n",
    "    model.add(keras.layers.Dense(1, **options))\n",
    "    optimizer = keras.optimizers.SGD(learning_rate)\n",
    "    model.compile(loss=\"mse\", optimizer=optimizer)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FUCKING piece of shit not saving again. fuck this shit. die in hell bitches"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
