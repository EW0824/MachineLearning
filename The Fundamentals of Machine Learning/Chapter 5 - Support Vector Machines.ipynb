{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very powerful and versatile, capable of both linear or nonlinear classificatino, regression and even outliter detection. Very popular and a must-have. \n",
    "\n",
    "Suited for classification of complex but small or medium sized datasets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "# Concepts not sure about:\n",
    "# Pipeline - ?\n",
    "# Feature scaling - standard scaler, etc\n",
    "# Polynomial features\n",
    "\n",
    "# Polynomial Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Polynomial Regression:\n",
    "\n",
    "Linear Regression - y = b0 + b1x1 <br>\n",
    "Multiple Linear Regression - y = b0 + b1x1 + b2x2 + ... + bnxn <br>\n",
    "Polynomial Linear Regression - y = b0 + b1x1 + b2x1^2 + b3x1^3 ... bnx1^3 <br>\n",
    "\n",
    "Instead of xn we have x1 to the power of n - only one variable but with different powers. Used whenever the data cannot be described using a straight line but can by a simple curve. The reason why its still linear is that what determines linear and non-linear, is not x, but instead the coeffecient. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Classification\n",
    "\n",
    "Unlike other models who might fit the decision boundary right next to data, and hence can't generalize well; \n",
    "\n",
    "SVM classifier not only separates the two classes but does so staying away from the closest training instances. It tries to fit the largest possible street between the classess - called Large margin classification. We want the margins to be free and as wide as possible. \n",
    "\n",
    "Adding instances 'off the street' or elsewhere won't affect the decision boundary - it's determined (supported) by the instances located on the edge of the street - called the support vectors. \n",
    "\n",
    "Again, feature scaling is very important - SVMs are very sensitive to scales. \n",
    "\n",
    "\n",
    "## Soft Margin Classification\n",
    "\n",
    "Hard margin classification - we impose that all instances be off the street on the right side. \n",
    "\n",
    "This only works if the data is linearly separable, and is very sensitive to outliers. Hard margin classification does not generalize well and becomes completely messed up with outliers.\n",
    "\n",
    "Soft margin classification, on the other hand, is much more flexible, and aims to find a good balance between keeping the street as large as possible, and limiting the margin violations (instances that end up in the middle or on the wrong side). \n",
    "\n",
    "The C parameter in Sklearn controls this - a smaller C value leads to a wider street but more margin violations, and vice versa. It seems like a wider street is preferrable as it will generalize better, as long as the margin violations are not on the wrong sides. If the SVM model is overfitting, reducing C might help. \n",
    "\n",
    "The LinearSVC class regularizes the bias term, so the training set should be centered first by subtracting its mean. StandardSlcaer does this for you. Setting the loss hyperparameter to 'hinge' is also important; finally, dual hyperparameter should be set to False for performance, unless there are more features than training instances. \n",
    "\n",
    "We continue to work on the Iris Dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('linear_svc', LinearSVC(C=1, loss='hinge'))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris = datasets.load_iris()\n",
    "X = iris['data'][:, (2, 3)] # petal length, petal width\n",
    "y = (iris['target'] == 2).astype(np.float64) # Iris-Virginica\n",
    "\n",
    "svm_clf = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('linear_svc', LinearSVC(C=1, loss='hinge'))\n",
    "])\n",
    "\n",
    "svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_clf.predict([[5.5, 17]]) # Not a probability, unlike the Logist Regression classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another approcach to this could be using the SVC class - SVC(kernel='linear', C=1), but it's very slow so not recommended. \n",
    "\n",
    "SGDClassifier class's SGDClassifier(loss='hinge', alpha=1/(m*C)) also works, which applies regular Stochastic Gradient Descent to train a linear SVM classifier. Doesn't converge as fast as the LinearSVC, but can be used for huge datasets or online classification tasks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nonlinear SVM Classification\n",
    "\n",
    "To handle nonlinear datasets, one approach is to add more features like polynomial features (Chapter 4).\n",
    "\n",
    "This can be implemented by creating a Pipeline containing a PolynomialFeatures transformer (pg130 on book), followed by a StandardScaler and a LinearSVC. \n",
    "\n",
    "We are using the moons dataset - a toy dataset for binary classification where the data points are shaped as two interleaving half circles. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('poly_features', PolynomialFeatures(degree=3)),\n",
       "                ('sclaer', StandardScaler()),\n",
       "                ('svm_clf', LinearSVC(C=10, loss='hinge'))])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "polynomial_svm_clf = Pipeline([\n",
    "    ('poly_features', PolynomialFeatures(degree=3)),\n",
    "    ('sclaer', StandardScaler()),\n",
    "    ('svm_clf', LinearSVC(C=10, loss='hinge'))\n",
    "])\n",
    "\n",
    "polynomial_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding polynomial features is simple and works great with all sorts of algorithms. However, at a low degree it cannot deal with very complex datasets, and at an overly high polynomial degree too many features are generated (since they are created combinatorially) to make the model too slow. \n",
    "\n",
    "Luckily, the mathematical technique of kernel trick can achieve the same result as if many polynomial features were added, without adding them and having too many features. \n",
    "\n",
    "Decrease the polynomial degree when overfitting. Converly, increase it when underfitting. (Increasing the degree makes the curves more fit to the training data (low bias), but runs the risk of overfitting with high variance).\n",
    "\n",
    "A grid search is common to find the right hyperparameter values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=5, coef0=1, kernel='poly'))])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "poly_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='poly', degree=3, coef0=1, C=5))\n",
    "])\n",
    "\n",
    "poly_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Features\n",
    "\n",
    "For nonlinear problems, you can also add features computed using a similarity function, measuring how much each instance resembles a particular landmark. \n",
    "\n",
    "To select the landmarks, simply create one at the location of every instance in the dataset, increasing the chance that the trianing set will be linearly separable. There is a risk of having too many features, nonetheless. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian RBF Kernel\n",
    "\n",
    "The similiatiry features method can be useful for all ML algorithms, but it might be computationally expensive, especially on large training sets. The kernel, again, enables you to obtain similar resuls without having to actually add the many similarity features. \n",
    "\n",
    "In the example below, increasing gamma makes the curve narrower - making the decision boundary more irregular and more fitting to the data. A small gamma value increases the curve, with a large range of influences and more smooth decision boundaries. \n",
    "\n",
    "y acts like a regularization hyperparameter - reduce if overfitting, increase if underfitting (similar to C)\n",
    "\n",
    "##### How to choose which kernel to use?\n",
    "Two kernels have been shown - others do exist but are used less widely. <br>\n",
    "A rule of thumb - start with the linear kernel (LinearSVC is much faster than SVC(kernel='linear)), especially on large training sets. If the set isn't as big, try Gaussian RBF which tends to work well. With spare computing power left, you can experiment with other kernels using cross-validation and grid search, especially for the specialized kernels for the training set's data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('scaler', StandardScaler()),\n",
       "                ('svm_clf', SVC(C=0.001, gamma=5))])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rbf_kernel_svm_clf = Pipeline([\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('svm_clf', SVC(kernel='rbf', gamma=5, C=0.001))\n",
    "])\n",
    "\n",
    "rbf_kernel_svm_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computer Complexity\n",
    "\n",
    "LinearSVC doesn't support kernel but training time is almost linear with the number of training instances. The tolerance hyperparameter (tol) controls the precision. \n",
    "\n",
    "The SVC class, supports the kernel trick but become dreadfully slow with large training sets - making it perfect for small/medium training sets. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Regression\n",
    "\n",
    "SVM supports classification. It also supports regression. VERY VERSATILE. \n",
    "\n",
    "Instead of trying to fit the largest street between two classes, regression tries to fit as many instances as possible on the street while limiting margin violations. The tolerance hyperparemter controls the width of the street. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVR(C=100, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Linear regression\n",
    "from sklearn.svm import LinearSVR\n",
    "\n",
    "svm_reg = LinearSVR(epsilon=1.5)\n",
    "svm_reg.fit(X, y)\n",
    "\n",
    "# Nonlinear regression\n",
    "from sklearn.svm import SVR # SVC equivalent in regression\n",
    "\n",
    "svm_poly_reg = SVR(kernel='poly', degree=2, C=100, epsilon=0.1)\n",
    "svm_poly_reg.fit(X, y)\n",
    "\n",
    "# LinearSVR and SVR behave exactly like their classification brothers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Under the Hood\n",
    "\n",
    "Will come back later. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
