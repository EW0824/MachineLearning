{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Learning\n",
    "\n",
    "If you aggregate the predictions of a group of predictors (classifiers/regressors) - you will get better predictions than with the best individual predictor. A group of predictors is called an ensemble - Ensemble Learning technique, and the algorithm is thus called Ensemble Method. \n",
    "\n",
    "For Decision Trees, you can get them to train each on a different random subset of the training set, obtain individual predictions, and then predict the class that gets the most votes. \n",
    "\n",
    "###### Random Forest - One of the most powerful Machine Learning algorithms.\n",
    "\n",
    "Ensemble methods used more near the end of a project - after predictors are already built, to combine them into an even better predictor. ML competition winners usually involve seversl Ensemble methods. \n",
    "\n",
    "Bagging, boosting, stacking and others are discussed in this chapter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Voting Classifiers\n",
    "\n",
    "For instance, we might have already trained a few classifiers. To create an even better classifier, we can aggregate the predictions of each, and predict the one with most votes. Hard voting classifier - the majority vote method. \n",
    "\n",
    "This voting classifier performs better than each classifier in the ensemble. With enough learners, even if each classifier is weak and useless, the ensemble performs a lot better.\n",
    "\n",
    "This is due to the law of large numbers - with 1000 classifiers only 51% correct, the ensemble can perform better than 75% accuracy. However, since classifiers are trained on the same dataset, sometimes they make correlated errors, and the majority vote will be for the wrong class which reduces the accuracy. \n",
    "\n",
    "Ensembles therefore work best when the predictors are independent from one another as possible. To get diverse classifiers train them on different algorithms. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "# Casual X, y from the moon dataset\n",
    "X, y = make_moons(n_samples=500, noise=0.30, random_state=24)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VotingClassifier(estimators=[('lr', LogisticRegression(random_state=24)),\n",
       "                             ('rf', RandomForestClassifier(random_state=24)),\n",
       "                             ('svc', SVC(random_state=24))])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=24)\n",
    "rnd_clf = RandomForestClassifier(n_estimators=100, random_state=24)\n",
    "svm_clf = SVC(gamma=\"scale\", random_state=24)\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf)],\n",
    "    voting='hard')\n",
    "\n",
    "voting_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.848\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "# Each classifier's accuracy score\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf, voting_clf):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))\n",
    "    \n",
    "# Supposed to outperform the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Soft voting - for probability predictions, Scikit-Learn will predict the class with the highest class probability, averaged over all the individual classifiers. \n",
    "\n",
    "Usually higher performance than hard voting because it gives weight to highly confident votes.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression 0.848\n",
      "RandomForestClassifier 0.904\n",
      "SVC 0.896\n",
      "VotingClassifier 0.896\n"
     ]
    }
   ],
   "source": [
    "# Using soft voting - have to give svm a predict_proba hyperparameter - uses cross-validation\n",
    "svm_clf_proba = SVC(gamma='scale', probability=True, random_state=24)\n",
    "\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[('lr', log_clf), ('rf', rnd_clf), ('svc', svm_clf_proba)],\n",
    "    voting='soft')\n",
    "\n",
    "for clf in (log_clf, rnd_clf, svm_clf_proba, voting_clf_soft):\n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(clf.__class__.__name__, accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging and Pasting\n",
    "\n",
    "One way to get diverse classifiers is to use different training algorithms. Another is to use the same algorithm but train predictors on different subsets of the training set. \n",
    "\n",
    "Both bagging and pasting allow training instances to be sampled several times across multiple predictors, but only bagging allows the instances to be sampled several times by the same predictor. \n",
    "\n",
    "\n",
    "Bagging - sampling with replacement (bootstraping)<br>\n",
    "Pasting - sampling without replacement (pasting small votes of classification in large databases and online)\n",
    "\n",
    "\n",
    "After predictors are all trained, the aggregation function uses statistical mode (most frequent prediction, like hard voting) for classification, or the average for regression. Aggregation reduces both bias and variance. Generally, the ensemble has a similar bias but lower variance than a single predictor trained on the training set.\n",
    "\n",
    "Predictors can be trained and predictions be made all in parallel with each other. Bagging and pasting are extremely popular because they scale very well. \n",
    "\n",
    "Generally, bagging has lower variance while pasting has less bias. Overall, bagging tends to result in better models. Cross-validation can also be preformed to compare both. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    max_samples=100, bootstrap=True, n_jobs=-1) # auto performs soft voting when there is predict_proba method\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "y_pred = bag_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8853333333333333"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Out of bag Evaluation\n",
    "# On average 37% of the training instance is never touched. It could therefore be used as a validation set. \n",
    "\n",
    "bag_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(), n_estimators=500,\n",
    "    bootstrap=True, n_jobs=-1, oob_score=True)# Set oob_score to True for an auto oob evaluation\n",
    "\n",
    "bag_clf.fit(X_train, y_train)\n",
    "bag_clf.oob_score_ # Should predict accuracy score on the test set as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.912"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = bag_clf.predict(X_test)\n",
    "accuracy_score(y_test, y_pred) # Close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Patches and Random Subspaces\n",
    "\n",
    "BaggingClassifier also supports sampling features, other than instances. Each predictor is trained on a random subset of the input features. Useful with high-dimensional inputs (images). \n",
    "\n",
    "Random Patches - the method of training both instances and features <br>\n",
    "Random Subspaces - the method of keeping all training instances and only sampling features. This is done by (bootstrap=False, max_samples=1.0) and (bootstrap_features=True and/or max_features smaller than 1.0)\n",
    "\n",
    "Result - sampling features creates predictor diversity - higher bias but lower variance. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "\n",
    "Ensemble of decision trees generally trained via bagging, typically with max_samples = 1.0 (size of the entire training set). \n",
    "\n",
    "Randomness when growing trees. Instead of searching for the best feature when splitting a node, it searches for the best feature among a random subset of them. Results in greater tree diversity - trading higher bias for a lower variance; which generally produces a better model. \n",
    "\n",
    "One good quality of Random Forests is how easy it is to measure the relative importance of each feature. It measures a feature's importance by looking at how much the tree nodes using the feature reduce impurity on weighted average (across all trees in forest). Each node's weight is equil to its number of training samples. Scikit-learn does this automatically, scaling the sum to 1. Accessible via feature_importances_ variable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, max_leaf_nodes=16, n_jobs=-1)\n",
    "rnd_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_rf = rnd_clf.predict(X_test)\n",
    "\n",
    "# Equivalent to:\n",
    "bag_rnd_clf = BaggingClassifier(\n",
    "    DecisionTreeClassifier(splitter='random', max_leaf_nodes=16),\n",
    "    n_estimators=500, max_samples=1.0, bootstrap=True, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sepal length (cm) 0.09455583982195351\n",
      "sepal width (cm) 0.02334699878827842\n",
      "petal length (cm) 0.45526036950717214\n",
      "petal width (cm) 0.426836791882596\n"
     ]
    }
   ],
   "source": [
    "# Feature Importance\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "rnd_clf = RandomForestClassifier(n_estimators=500, n_jobs=-1)\n",
    "rnd_clf.fit(iris['data'], iris['target'])\n",
    "\n",
    "for name, score in zip(iris['feature_names'], rnd_clf.feature_importances_):\n",
    "    print(name, score)\n",
    "    \n",
    "# Very handy to know what features matter, if feature selection is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting (hypothesis boosting)\n",
    "\n",
    "Ensemble method that combines several weak learners into a strong learner. To train predictors sequentially, each trying to correct its predecessor. Many different methods. \n",
    "\n",
    "Most popular techniques are AdaBoost (adaptive boosting), and Gradient Boosting. \n",
    "\n",
    "### AdaBoost \n",
    "\n",
    "One way to get better is to pay more attention to the instances that the predecessor underfitted - focusing more and more on the hard cases. \n",
    "\n",
    "To build an AdaBoost classifier, a base classifier is trained and makes predictions. The relative weight of misclassified training instances is then increased. A second classifier is trained with the updated weights, and so on. \n",
    "\n",
    "The sequential techniques are similiar with Gradient Descent, except that instead of tweaking a single predictor's parameters to minimize the cost function, AdaBoost adds predictors to the ensemble and makes it better at each iteration. \n",
    "\n",
    "After training predictions are made similar to bagging/pasting, but each predictor have a different weight depending on their accuracy. A significant drawback, therefore, is that predictions cannot be parallelized, and therefore does not scale as well.\n",
    "\n",
    "SKLearn uses the multiclass variation of AdaBoost called SAMME. It behaves normally unless the predictors can estimate class probabilities (predict_proba), in whicn case it uses class probabilities and tend to perform better. \n",
    "\n",
    "If AdaBoost is overfitting, simply reduce the number of estimators, or regularize the base one more strongly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaBoostClassifier(base_estimator=DecisionTreeClassifier(max_depth=1),\n",
       "                   learning_rate=0.5, n_estimators=200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "ada_clf = AdaBoostClassifier(\n",
    "    DecisionTreeClassifier(max_depth=1), n_estimators=200,\n",
    "    algorithm='SAMME.R', learning_rate = 0.5)\n",
    "\n",
    "ada_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting\n",
    "\n",
    "As another sequential trainer algorithm, instead of tweaking the instance weights at every iteration, Gradient Boosting tries to fit the new predictor to the residual errors made by the previous one. \n",
    "\n",
    "To train a Gradient Tree Boosting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All from the official notebook. !!! Learn to generate random datasets!\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) - 0.5\n",
    "y = 3*X[:, 0]**2 + 0.05 * np.random.randn(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to train with gradient boosting - the old way\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "tree_reg1 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg1.fit(X, y)\n",
    "\n",
    "y2 = y - tree_reg1.predict(X)\n",
    "tree_reg2 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg2.fit(X, y2)\n",
    "\n",
    "y3 = y2 - tree_reg2.predict(X)\n",
    "tree_reg3 = DecisionTreeRegressor(max_depth=2)\n",
    "tree_reg3.fit(X, y3)\n",
    "\n",
    "X_new = np.array([[0.8]])\n",
    "\n",
    "# The ensemble of 3 trees, makes predictions simply by adding up the predictions of all 3\n",
    "# y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))\n",
    "y_pred = sum(tree.predict(X_new) for tree in (tree_reg1, tree_reg2, tree_reg3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(learning_rate=1.0, max_depth=2, n_estimators=3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simpler way\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "grbt = GradientBoostingRegressor(max_depth=2, n_estimators=3, learning_rate=1.0)\n",
    "grbt.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning_rate hyperparameter (0 to 1) scales the contribution of each tree. A low value means more trees are needed to fit the training set, but the predictions will generalize better. This regularization technique is called shrinkage. \n",
    "\n",
    "To find the optimal number of trees, early stopping is good. To do this is to use the staged_predict() method, which returns an iterator over the predictions made by the ensemble at each stage of training (1, 2, 3 trees, etc). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=2, n_estimators=84)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This trains a large number of trees before looking back to find the optimal number\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, n_estimators=120)\n",
    "gbrt.fit(X_train, y_train)\n",
    "\n",
    "errors = [mean_squared_error(y_val, y_pred)\n",
    "          for y_pred in gbrt.staged_predict(X_val)]\n",
    "best_n_estimators = np.argmin(errors) # indices of minimums\n",
    "\n",
    "gbrt_best = GradientBoostingRegressor(max_depth=2, n_estimators=best_n_estimators)\n",
    "gbrt_best.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Actually stopping training early - stops when validation error does not improve for five iterations in a row\n",
    "\n",
    "gbrt = GradientBoostingRegressor(max_depth=2, warm_start=True)\n",
    "\n",
    "min_val_error = float('inf')\n",
    "error_going_up = 0\n",
    "\n",
    "for n_estimators in range(1, 120):\n",
    "    gbrt.n_estimators = n_estimators\n",
    "    gbrt.fit(X_train, y_train)\n",
    "    y_pred = gbrt.predict(X_val)\n",
    "    val_error = mean_squared_error(y_val, y_pred)\n",
    "    if val_error < min_val_error:\n",
    "        min_val_error = val_error\n",
    "        error_going_up = 0\n",
    "    else:\n",
    "        error_going_up += 1\n",
    "        if error_going_up == 5:\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic Gradient Boosting - setting the subsample for each tree to be trained on a fraction of the instances selected randomly. This trades bias for variance and speeds up training as well. \n",
    "\n",
    "XGBoost - the python library containing optimized implementation of Gradient Boosting  It's extrmeley fast, scalable and portable. An important component of the winning entries in ML competitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-d18d7469fa18>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mxgb_reg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgboost\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBRegressor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb_reg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import xgboost\n",
    "\n",
    "xgb_reg = xgboost.XGBRegressor()\n",
    "xgb_reg.fit(X_train, y_train)\n",
    "y_pred = xgb_reg.predict(X_val)\n",
    "\n",
    "# Taking care of early stopping automatically \n",
    "xgb_reg.fit(X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)], early_stopping_rounds=2)\n",
    "y_pred = xgb_reg.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking (stacked generalization)\n",
    "\n",
    "Instead of using trivial functions (like hard voting) to\n",
    "aggregate the predictions, why don't we train a model? Each predictor would predict a value, passed as inputs as the blender wh produces the final prediction. \n",
    "\n",
    "A common approach - hold-out set. We divide the training set in half, and use the first half to train the predictors at the first layer. They are then made to make predictions on the second half. A training set can thus be created, using the predictions as input features and keeping the target values. The blender is trained on this training set, learning to predict the target value given the first layer's predictions. We can train layers of blenders, simply by dividing the training set into more subsets. \n",
    "\n",
    "Scikit-Learn does not support stacking. It's not hard to self-learn, but open-source implementations like brew are also good. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
